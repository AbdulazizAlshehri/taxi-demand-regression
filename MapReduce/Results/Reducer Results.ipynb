{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialisation\t took 0s\n",
      "split rows\t took 97s\n",
      "No. rows in training dataset:  37561189\n",
      "No. rows in test dataset:  33722144\n",
      "convert rows to dataframes\t took 67s\n",
      "training cluster model\t took 36s\n",
      "predicting clusters\t took 69s\n",
      "INFO: Pandarallel will run on 24 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "reformat time\t took 597s\n",
      "grouping and calculate pickups\t took 4s\n",
      "pickups to percentage\t took 24s\n",
      "reformat time\t took 0s\n",
      "spliting to train and test\t took 0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/anaconda/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training models\t took 13s\n",
      "\n",
      "Linear Regression:\n",
      "+-------------+----------------------+\n",
      "|  Experiment |        Value         |\n",
      "+-------------+----------------------+\n",
      "|      R2     | 0.12358378989809549  |\n",
      "| Adjusted R2 | 0.12351541283175582  |\n",
      "|     MSE     | 0.022728725137356843 |\n",
      "|     RMSE    | 0.15076048931121458  |\n",
      "+-------------+----------------------+\n",
      "\n",
      "RandomForest:\n",
      "+-------------+-----------------------+\n",
      "|  Experiment |         Value         |\n",
      "+-------------+-----------------------+\n",
      "|      R2     |   0.9121364300505471  |\n",
      "| Adjusted R2 |   0.9121295750276915  |\n",
      "|     MSE     | 0.0022786284734918785 |\n",
      "|     RMSE    |  0.04773498165383411  |\n",
      "+-------------+-----------------------+\n",
      "\n",
      "XGB:\n",
      "+-------------+----------------------+\n",
      "|  Experiment |        Value         |\n",
      "+-------------+----------------------+\n",
      "|      R2     |  0.9190748065302935  |\n",
      "| Adjusted R2 |  0.919068492832237   |\n",
      "|     MSE     | 0.002098690619661767 |\n",
      "|     RMSE    | 0.045811468211156114 |\n",
      "+-------------+----------------------+\n",
      "printing results\t took 0s\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"A more advanced Reducer, using Python iterators and generators.\"\"\"\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from datetime import datetime, timedelta\n",
    "from sklearn.cluster import MiniBatchKMeans, KMeans\n",
    "from pandarallel import pandarallel # use: pip install pandarallel \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from xgboost import XGBRegressor # use: conda install xgboost\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "from prettytable import PrettyTable\n",
    "import gc\n",
    "import fileinput\n",
    "start_time = time.time()\n",
    "i = 0\n",
    "    \n",
    "def model_evaluation(algorithem_name, X_Test, y_pred, y_true):\n",
    "    \n",
    "    # R2 and Adjasted R2\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    adj_r2 = 1-(1-r2)*((len(X_Test)-1)/(len(X_Test)-X_Test.shape[1]-1))\n",
    "    # MSE and RMSE\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mse)\n",
    "    \n",
    "    # print in table\n",
    "    table = PrettyTable()\n",
    "    print('\\n' + algorithem_name + ':')\n",
    "    table.field_names = ['Experiment', 'Value']\n",
    "    table.add_row(['R2', r2])\n",
    "    table.add_row(['Adjusted R2', adj_r2])\n",
    "    table.add_row(['MSE',mse])\n",
    "    table.add_row(['RMSE', rmse])\n",
    "    return table\n",
    "\n",
    "\n",
    "def measure_time(label = ''):\n",
    "    global start_time\n",
    "    global i\n",
    "    print('{}\\t took {}s'.format(label, round(time.time() - start_time)))\n",
    "    start_time = time.time()\n",
    "    i+=1\n",
    "    \n",
    "def read_mapper_output(file):\n",
    "    for line in file:\n",
    "        yield line\n",
    "\n",
    "def main(separator='\\t'):\n",
    "    \n",
    "    df_train = []\n",
    "    df_test = []\n",
    "    names=['pickup_time', 'pickup_longitude', 'pickup_latitude']\n",
    "    c=['tpep_pickup_datetime',\n",
    "           'pickup_longitude',\n",
    "           'pickup_latitude']\n",
    "    measure_time('initialisation')\n",
    "    \n",
    "\n",
    "        \n",
    "    # input comes from STDIN (standard input)\n",
    "    gc.disable()\n",
    "    \n",
    "    with fileinput.input(files=('/home/Aziz/MapperOutput_2015-01.csv', \n",
    "                                '/home/Aziz/MapperOutput_2015-02.csv', \n",
    "                                '/home/Aziz/MapperOutput_2015-03.csv', \n",
    "                                '/home/Aziz/MapperOutput_2016-01.csv', \n",
    "                                '/home/Aziz/MapperOutput_2016-02.csv', \n",
    "                                '/home/Aziz/MapperOutput_2016-03.csv'\n",
    "                               )) as f:\n",
    "        for line in f:\n",
    "            #data = read_mapper_output(sys.stdin)\n",
    "            for row in f:\n",
    "                #print(row)\n",
    "                key, row = row.split(separator)\n",
    "                row = row.split(',')\n",
    "                if key=='train':\n",
    "                    df_train.append(row)\n",
    "                elif key=='test':\n",
    "                    df_test.append(row)\n",
    "\n",
    "                    \n",
    "#     data = read_mapper_output(sys.stdin)\n",
    "#     for row in data:\n",
    "#         print(row)\n",
    "#         key, row = row.split(separator)\n",
    "#         row = row.split(',')\n",
    "#         if key=='train':\n",
    "#             df_train.append(row)\n",
    "#         elif key=='test':\n",
    "#             df_test.append(row)\n",
    "#     gc.enable()\n",
    "    measure_time('split rows') # measuring time for debugging purposes\n",
    "    \n",
    "    \n",
    "    \n",
    "    # converting rows of data into pandas dataframes\n",
    "    df_train = pd.DataFrame(df_train, columns=names)\n",
    "    df_test = pd.DataFrame(df_test, columns=names)\n",
    "    # for debug only\n",
    "    print('No. rows in training dataset:  %d' % df_train.shape[0])\n",
    "    print('No. rows in test dataset:  %d' % df_test.shape[0])\n",
    "    measure_time('convert rows to dataframes') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "\n",
    "    ## GEOGRAPHICAL SEGMENTATION BY CLUSTERING\n",
    "    #Clustering pickups, Getting clusters \n",
    "    coord = df_train[[\"pickup_latitude\", \"pickup_longitude\"]].values\n",
    "    regions = MiniBatchKMeans(n_clusters = 30, batch_size = 10000).fit(coord)\n",
    "    measure_time('training cluster model') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "    \n",
    "    #predecting clusters\n",
    "    df_train[\"pickup_cluster\"] = regions.predict(df_train[[\"pickup_latitude\", \"pickup_longitude\"]])\n",
    "    df_test[\"pickup_cluster\"] = regions.predict(df_test[[\"pickup_latitude\", \"pickup_longitude\"]])\n",
    "    measure_time('predicting clusters') # printing time for debugging only\n",
    "\n",
    "    \n",
    "    \n",
    "    # Replacing mins and sec with 0\n",
    "    pandarallel.initialize() \n",
    "    df_train['pickup_time'] = df_train.pickup_time.parallel_apply(lambda x : pd.to_datetime(x).replace(minute=0, second=0))\n",
    "    df_test['pickup_time'] = df_test.pickup_time.parallel_apply(lambda x : pd.to_datetime(x).replace(minute=0, second=0))\n",
    "    measure_time('reformat time') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## CALCULATING TAXI-DEMAND\n",
    "    # Group by Cluster_id and time\n",
    "    df_train_2 = df_train.groupby(['pickup_time','pickup_cluster']).size().reset_index(name='count')\n",
    "    df_test_2 = df_test.groupby(['pickup_time','pickup_cluster']).size().reset_index(name='count')\n",
    "    measure_time('grouping and calculate pickups') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Converting pickup counts to demand percentage\n",
    "    df_train_2['count'] = df_train_2['count'].parallel_apply(lambda x :  (x / df_train_2['count'].max()))\n",
    "    df_test_2['count'] = df_test_2['count'].parallel_apply(lambda x :  (x / df_test_2['count'].max()))\n",
    "    measure_time('pickups to percentage') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Getting month, days, hours, day of week\n",
    "    df_train_2['month'] = pd.DatetimeIndex(df_train_2['pickup_time']).month\n",
    "    df_train_2['day'] = pd.DatetimeIndex(df_train_2['pickup_time']).day\n",
    "    df_train_2['dayofweek'] = pd.DatetimeIndex(df_train_2['pickup_time']).dayofweek\n",
    "    df_train_2['hour'] = pd.DatetimeIndex(df_train_2['pickup_time']).hour\n",
    "    \n",
    "    df_test_2['month'] = pd.DatetimeIndex(df_test_2['pickup_time']).month\n",
    "    df_test_2['day'] = pd.DatetimeIndex(df_test_2['pickup_time']).day\n",
    "    df_test_2['dayofweek'] = pd.DatetimeIndex(df_test_2['pickup_time']).dayofweek\n",
    "    df_test_2['hour'] = pd.DatetimeIndex(df_test_2['pickup_time']).hour\n",
    "    measure_time('reformat time') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "    \n",
    "    # X and y for training\n",
    "    X_train = df_train_2[['pickup_cluster', 'month', 'day', 'hour', 'dayofweek']]\n",
    "    y_train = df_train_2['count']\n",
    "\n",
    "    # X and y for testing\n",
    "    X_test = df_test_2[['pickup_cluster', 'month', 'day', 'hour', 'dayofweek']]\n",
    "    y_test = df_test_2['count']\n",
    "    measure_time('spliting to train and test') # printing time for debugging only\n",
    "\n",
    "    \n",
    "    # split training data into training and validation data\n",
    "    #X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, \n",
    "    #                                                  test_size=0.33, random_state=42)\n",
    "    #measure_time('spliting train and validation') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Linear regression\n",
    "    LReg = LinearRegression()\n",
    "    LReg.fit(X_train, y_train)\n",
    "    LReg_y_pred = LReg.predict(X_test)\n",
    "    \n",
    "    # RandomForest regression\n",
    "    RFRegr = RandomForestRegressor()\n",
    "    RFRegr.fit(X_train, y_train)\n",
    "    RFRegr_y_pred = RFRegr.predict(X_test)\n",
    "    \n",
    "    # XGB regression\n",
    "    GBRegr = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "    GBRegr.fit(X_train, y_train)\n",
    "    GBRegr_y_pred = GBRegr.predict(X_test)\n",
    "    measure_time('training models') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "    \n",
    "    # evaluation\n",
    "    #print(model_evaluation('y True',X_Test=X_test, y_pred=y_test, y_true=y_test))\n",
    "    print(model_evaluation('Linear Regression',X_Test=X_test, y_pred=LReg_y_pred, y_true=y_test))\n",
    "    print(model_evaluation('RandomForest',X_Test=X_test, y_pred=RFRegr_y_pred, y_true=y_test))\n",
    "    print(model_evaluation('XGB',X_Test=X_test, y_pred=GBRegr_y_pred, y_true=y_test))\n",
    "    measure_time('printing results') # printing time for debugging only\n",
    "    \n",
    "    \n",
    "    \n",
    "    #print(\"--- %s seconds --- 0\" % (time.time() - start_time))\n",
    "    #display(X_train)\n",
    "    #display(y_train)\n",
    "    #display(X_validation)\n",
    "    #display(y_validation)\n",
    "    #display(X_test)\n",
    "    #display(y_test)\n",
    "    \n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sys.stdin = open('/home/Aziz/MapperOutput.txt','r')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
